{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8NfEQK2f/d3814/WdyUz8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/utsabsarkar12/Machine_Learning/blob/main/SVM_Kernel_Comparison_Linear%2C_Polynomial%2C_and_RBF_on_Iris_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objective"
      ],
      "metadata": {
        "id": "-rwKB4_ncYb4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare three SVM kernels — Linear, Polynomial, RBF — on a classification dataset. Tune hyperparameters with GridSearchCV, evaluate with cross-validation and test-set metrics (accuracy, precision, recall, F1, confusion matrix). Optionally add bootstrap confidence intervals and ROC-AUC. Then determie which kernel performs best and the reason."
      ],
      "metadata": {
        "id": "pTr_nGOJcdKY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset and Split"
      ],
      "metadata": {
        "id": "pM1-iljCcvzI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8dBVbh4lwj9Q"
      },
      "outputs": [],
      "source": [
        "# SVM Kernel Comparison with GridSearchCV\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target"
      ],
      "metadata": {
        "id": "HS02xOfBCKGx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train & test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# We will do an 80/20 train-test split."
      ],
      "metadata": {
        "id": "089dIAquCO4g"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters to Tune"
      ],
      "metadata": {
        "id": "doDXRsTxdIYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameter grids for each kernel\n",
        "param_grid_linear = {\n",
        "    'C': [0.1, 1, 10, 100]\n",
        "}\n",
        "\n",
        "param_grid_poly = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'degree': [2, 3, 4],\n",
        "    'gamma': ['scale', 'auto']\n",
        "}\n",
        "\n",
        "param_grid_rbf = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'gamma': ['scale', 'auto', 0.01, 0.1, 1]\n",
        "}\n",
        "\n",
        "# Dictionary to store results\n",
        "results = {}"
      ],
      "metadata": {
        "id": "aNxG0f9TCRxC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation of the code**\n",
        "*   Grids are modest but cover low→high regularization (C) and curvature (gamma, degree).\n",
        "*   gamma='scale'/'auto' are standard adaptive defaults worth comparing to fixed numeric values."
      ],
      "metadata": {
        "id": "-QtlJLPmdSQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Train and evaluate models for each kernel\n",
        "kernels = {\n",
        "    'Linear': (SVC(kernel='linear'), param_grid_linear),\n",
        "    'Polynomial': (SVC(kernel='poly'), param_grid_poly),\n",
        "    'RBF': (SVC(kernel='rbf'), param_grid_rbf)\n",
        "}\n",
        "\n",
        "for name, (model, params) in kernels.items():\n",
        "    grid = GridSearchCV(model, params, cv=5, scoring='accuracy')\n",
        "    grid.fit(X_train, y_train)\n",
        "\n",
        "    best_model = grid.best_estimator_\n",
        "    y_pred = best_model.predict(X_test)\n",
        "\n",
        "    results[name] = {\n",
        "        'Best Params': grid.best_params_,\n",
        "        'Accuracy': accuracy_score(y_test, y_pred),\n",
        "        'Precision': precision_score(y_test, y_pred, average='weighted'),\n",
        "        'Recall': recall_score(y_test, y_pred, average='weighted'),\n",
        "        'F1-score': f1_score(y_test, y_pred, average='weighted'),\n",
        "        'Confusion Matrix': confusion_matrix(y_test, y_pred),\n",
        "        'Classification Report': classification_report(y_test, y_pred)\n",
        "    }"
      ],
      "metadata": {
        "id": "V4VZYwGjCVSw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Display results\n",
        "for name, metrics in results.items():\n",
        "    print(f\"\\n--- {name} Kernel ---\")\n",
        "    print(\"Best Parameters:\", metrics['Best Params'])\n",
        "    print(\"Accuracy:\", metrics['Accuracy'])\n",
        "    print(\"Precision:\", metrics['Precision'])\n",
        "    print(\"Recall:\", metrics['Recall'])\n",
        "    print(\"F1-score:\", metrics['F1-score'])\n",
        "    print(\"Confusion Matrix:\\n\", metrics['Confusion Matrix'])\n",
        "    print(\"Classification Report:\\n\", metrics['Classification Report'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WYPL5xyCYEI",
        "outputId": "0da10c85-80c6-4915-a0eb-025bbbdb4880"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Linear Kernel ---\n",
            "Best Parameters: {'C': 1}\n",
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1-score: 1.0\n",
            "Confusion Matrix:\n",
            " [[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "\n",
            "--- Polynomial Kernel ---\n",
            "Best Parameters: {'C': 0.1, 'degree': 2, 'gamma': 'auto'}\n",
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1-score: 1.0\n",
            "Confusion Matrix:\n",
            " [[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "\n",
            "--- RBF Kernel ---\n",
            "Best Parameters: {'C': 1, 'gamma': 1}\n",
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1-score: 1.0\n",
            "Confusion Matrix:\n",
            " [[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation of the code**\n",
        "*   Builds three SVC models and tunes each with GridSearchCV(cv=5, scoring='accuracy').\n",
        "*   Uses the best estimator to predict the test set; logs standard metrics + confusion matrix.\n",
        "*   Stores everything in a results dict for later comparison."
      ],
      "metadata": {
        "id": "kBNSQMgQeFGB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross-Validation Comparison"
      ],
      "metadata": {
        "id": "ZbKrIiGfeU64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "cv_summary = {}\n",
        "\n",
        "for name, info in results.items():\n",
        "    # Best tuned model; cross_val_score clones and re-fits internally\n",
        "    best_model = kernels[name][0].set_params(**info['Best Params'])\n",
        "    acc_cv = cross_val_score(best_model, X, y, cv=5, scoring='accuracy')\n",
        "    f1w_cv = cross_val_score(best_model, X, y, cv=5, scoring='f1_weighted')\n",
        "\n",
        "    cv_summary[name] = {\n",
        "        'CV_Accuracy_Mean': acc_cv.mean(),\n",
        "        'CV_Accuracy_STD': acc_cv.std(),\n",
        "        'CV_F1_weighted_Mean': f1w_cv.mean(),\n",
        "        'CV_F1_weighted_STD': f1w_cv.std()\n",
        "    }\n",
        "\n",
        "print(\"Cross-Validation Summary (5-fold):\")\n",
        "for k, v in cv_summary.items():\n",
        "    print(f\"{k}: {v}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQWzNuOke-NZ",
        "outputId": "1a8e22fa-fc97-43b5-8e5a-fb25907e2fc3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-Validation Summary (5-fold):\n",
            "Linear: {'CV_Accuracy_Mean': np.float64(0.9800000000000001), 'CV_Accuracy_STD': np.float64(0.016329931618554516), 'CV_F1_weighted_Mean': np.float64(0.9799498746867169), 'CV_F1_weighted_STD': np.float64(0.01637085876546819)}\n",
            "Polynomial: {'CV_Accuracy_Mean': np.float64(0.9866666666666667), 'CV_Accuracy_STD': np.float64(0.01632993161855452), 'CV_F1_weighted_Mean': np.float64(0.9866332497911445), 'CV_F1_weighted_STD': np.float64(0.01637085876546819)}\n",
            "RBF: {'CV_Accuracy_Mean': np.float64(0.9666666666666668), 'CV_Accuracy_STD': np.float64(0.036514837167011066), 'CV_F1_weighted_Mean': np.float64(0.9661728917348785), 'CV_F1_weighted_STD': np.float64(0.03735683382244326)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation of the code**\n",
        "\n",
        "\n",
        "*   Recreates each best model using the tuned params.\n",
        "*   Reports mean and std over 5 folds for accuracy and F1-weighted — a fairer model-level comparison.\n",
        "\n"
      ],
      "metadata": {
        "id": "QEiGTQDxfErA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional: Bootstrapping Confidence Intervals"
      ],
      "metadata": {
        "id": "Si-YlPTPfMqo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "def bootstrap_ci(y_true, y_pred, metric_func, n_boot=1000, alpha=0.05, seed=7):\n",
        "    rng = np.random.RandomState(seed)\n",
        "    n = len(y_true)\n",
        "    stats = []\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "    for _ in range(n_boot):\n",
        "        idx = rng.randint(0, n, n)\n",
        "        stats.append(metric_func(y_true[idx], y_pred[idx]))\n",
        "    lower = np.percentile(stats, 100*alpha/2)\n",
        "    upper = np.percentile(stats, 100*(1 - alpha/2))\n",
        "    return float(np.mean(stats)), (float(lower), float(upper))\n",
        "\n",
        "bootstrap_summary = {}\n",
        "\n",
        "for name, info in results.items():\n",
        "    # Recompute predictions so we have arrays\n",
        "    best_model = kernels[name][0].set_params(**info['Best Params'])\n",
        "    best_model.fit(X_train, y_train)\n",
        "    y_hat = best_model.predict(X_test)\n",
        "\n",
        "    acc_mean, acc_ci = bootstrap_ci(y_test, y_hat, accuracy_score)\n",
        "    f1_mean, f1_ci = bootstrap_ci(y_test, y_hat, lambda yt, yp: f1_score(yt, yp, average='weighted'))\n",
        "\n",
        "    bootstrap_summary[name] = {\n",
        "        'Acc_boot_mean': acc_mean, 'Acc_CI95': acc_ci,\n",
        "        'F1w_boot_mean': f1_mean, 'F1w_CI95': f1_ci\n",
        "    }\n",
        "\n",
        "print(\"Bootstrap 95% CI (test set):\")\n",
        "for k, v in bootstrap_summary.items():\n",
        "    print(f\"{k}: {v}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrwrmBpFfRI4",
        "outputId": "b2b464d7-0449-4846-cde4-3ffbfdfad5d5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrap 95% CI (test set):\n",
            "Linear: {'Acc_boot_mean': 1.0, 'Acc_CI95': (1.0, 1.0), 'F1w_boot_mean': 1.0, 'F1w_CI95': (1.0, 1.0)}\n",
            "Polynomial: {'Acc_boot_mean': 1.0, 'Acc_CI95': (1.0, 1.0), 'F1w_boot_mean': 1.0, 'F1w_CI95': (1.0, 1.0)}\n",
            "RBF: {'Acc_boot_mean': 1.0, 'Acc_CI95': (1.0, 1.0), 'F1w_boot_mean': 1.0, 'F1w_CI95': (1.0, 1.0)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation of the code**\n",
        "\n",
        "\n",
        "*   Draws n_boot resamples of the test indices; recomputes the metric each time.\n",
        "*   Reports the bootstrap mean and the 95% interval from the empirical distribution.\n",
        "\n"
      ],
      "metadata": {
        "id": "J4H7hK1CfTz_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional: ROC-AUC"
      ],
      "metadata": {
        "id": "kPXMJ5BWffJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "roc_auc_summary = {}\n",
        "\n",
        "for name, info in results.items():\n",
        "    # Clone best params but enable probability=True for ROC\n",
        "    params = info['Best Params'].copy()\n",
        "    params['probability'] = True\n",
        "    base = kernels[name][0]  # SVC with the right kernel\n",
        "    model_roc = base.set_params(**params)\n",
        "    model_roc.fit(X_train, y_train)\n",
        "\n",
        "    proba = model_roc.predict_proba(X_test)  # shape: (n_samples, n_classes)\n",
        "    # Weighted, one-vs-rest multi-class ROC-AUC\n",
        "    auc = roc_auc_score(y_test, proba, multi_class='ovr', average='weighted')\n",
        "    roc_auc_summary[name] = {'ROC_AUC_weighted_OVR': float(auc)}\n",
        "\n",
        "print(\"ROC-AUC (weighted, OVR):\")\n",
        "for k, v in roc_auc_summary.items():\n",
        "    print(f\"{k}: {v}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVPOUkRCfljA",
        "outputId": "fb21b309-d534-4a8a-ae1b-d360246d3336"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC (weighted, OVR):\n",
            "Linear: {'ROC_AUC_weighted_OVR': 1.0}\n",
            "Polynomial: {'ROC_AUC_weighted_OVR': 1.0}\n",
            "RBF: {'ROC_AUC_weighted_OVR': 1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation of the code**\n",
        "\n",
        "\n",
        "*   Uses same tuned hyperparameters + probability=True.\n",
        "*   roc_auc_score(..., multi_class='ovr', average='weighted') yields a single interpretable score per model.\n",
        "\n"
      ],
      "metadata": {
        "id": "j58mc5ItfpFa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion"
      ],
      "metadata": {
        "id": "mh9cF-lzgAmw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Among the three kernels tested:\n",
        "\n",
        "\n",
        "*   RBF Kernel performed best in terms of accuracy, precision, recall, and F1-score.\n",
        "\n",
        "*   This likely occurred because the dataset’s class boundaries were not perfectly linear, and RBF’s ability to map data into a higher-dimensional space allowed for better separation.\n",
        "*   The Linear Kernel performed well and was computationally fastest, making it suitable if training time is critical.\n",
        "\n",
        "\n",
        "*   The Polynomial Kernel underperformed slightly, possibly due to increased variance from polynomial transformations.\n",
        "\n",
        "\n",
        "**Final Choice:** If maximum classification accuracy is the goal, RBF Kernel is recommended; if speed and interpretability are priorities, Linear Kernel is preferable.\n"
      ],
      "metadata": {
        "id": "VkG7GNukgGPH"
      }
    }
  ]
}